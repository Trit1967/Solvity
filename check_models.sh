#!/bin/bash

echo "üìã Top Ollama Models Available:"
echo "================================"
echo ""
echo "üèÜ BEST MODELS (2024-2025):"
echo ""
echo "Small (2-4GB):"
echo "  ‚Ä¢ llama3.2      - Meta's latest, 3B params"
echo "  ‚Ä¢ phi3          - Microsoft, very efficient"
echo "  ‚Ä¢ gemma2:2b     - Google's smallest"
echo "  ‚Ä¢ qwen2.5:3b    - Alibaba's latest"
echo ""
echo "Medium (4-8GB):"
echo "  ‚Ä¢ llama3.1:8b   - Meta's best 8B model"
echo "  ‚Ä¢ mistral       - French, very good"
echo "  ‚Ä¢ gemma2:9b     - Google's medium"
echo "  ‚Ä¢ deepseek-coder:6.7b - For coding"
echo ""
echo "Large (8-16GB):"
echo "  ‚Ä¢ llama3.1:70b  - Meta's largest (quantized)"
echo "  ‚Ä¢ mixtral:8x7b  - Mixture of experts"
echo "  ‚Ä¢ qwen2.5:14b   - Alibaba's large"
echo "  ‚Ä¢ deepseek-llm:7b - DeepSeek's base model"
echo ""
echo "Huge (32GB+):"
echo "  ‚Ä¢ llama3.1:405b - Meta's giant (quantized)"
echo "  ‚Ä¢ mixtral:8x22b - Massive MoE"
echo ""
echo "To use any model:"
echo "  ollama pull <model-name>"
echo "  ollama run <model-name>"
echo ""
echo "DeepSeek models in Ollama:"
echo "  ‚Ä¢ deepseek-coder:6.7b"
echo "  ‚Ä¢ deepseek-coder:33b"
echo "  ‚Ä¢ deepseek-llm:7b"